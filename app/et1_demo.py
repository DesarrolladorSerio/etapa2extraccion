from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month, avg, count, when
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# ==============================================================
# üß† ETAPA 3 ‚Äì Preparaci√≥n del experimento
# ==============================================================
# Objetivo: dise√±ar y ejecutar un experimento que extraiga conocimiento
# relevante del dataset: tendencias y un modelo de regresi√≥n simple
# para predecir precios de vivienda seg√∫n atributos estructurales.
# ==============================================================

# 1Ô∏è‚É£ Inicializar Spark
spark = SparkSession.builder.appName("Etapa3_Experimento").getOrCreate()
print("\n=== üöÄ INICIO EXPERIMENTO ETAPA 3 ===\n")

# 2Ô∏è‚É£ Cargar dataset (extracci√≥n)
df = spark.read.csv("/data/pp-sample-10k.csv", header=True, inferSchema=True)
print(f"‚úÖ Dataset cargado: {df.count()} filas, {len(df.columns)} columnas")

# 3Ô∏è‚É£ Limpieza b√°sica (procesamiento)
# Mantener variables relevantes para predicci√≥n
df = df.select("Price", "Date", "PropertyType", "NewBuild", "Duration", "TownCity", "County")

# Eliminar filas sin datos cr√≠ticos
df = df.dropna(subset=["Price", "PropertyType", "County", "Date"])

# Convertir fecha a a√±o y mes
df = df.withColumn("Year", year(col("Date"))).withColumn("Month", month(col("Date")))

# Mostrar esquema final
df.printSchema()

# 4Ô∏è‚É£ An√°lisis exploratorio avanzado
print("\n=== üìä An√°lisis exploratorio ===")
df.groupBy("Year").agg(
    count("*").alias("Num_transacciones"),
    avg("Price").alias("Precio_promedio")
).orderBy("Year").show(10)

df.groupBy("County").agg(
    avg("Price").alias("Precio_promedio"),
    count("*").alias("Ventas")
).orderBy(col("Ventas").desc()).show(10)

# 5Ô∏è‚É£ Codificaci√≥n de variables categ√≥ricas
print("\n=== üîß Preparando variables para modelo ===")
indexers = {
    "PropertyType": StringIndexer(inputCol="PropertyType", outputCol="PropertyTypeIndex"),
    "NewBuild": StringIndexer(inputCol="NewBuild", outputCol="NewBuildIndex"),
    "Duration": StringIndexer(inputCol="Duration", outputCol="DurationIndex"),
    "County": StringIndexer(inputCol="County", outputCol="CountyIndex")
}

for key, indexer in indexers.items():
    df = indexer.fit(df).transform(df)

# 6Ô∏è‚É£ Definici√≥n del conjunto de caracter√≠sticas
assembler = VectorAssembler(
    inputCols=["PropertyTypeIndex", "NewBuildIndex", "DurationIndex", "CountyIndex", "Year", "Month"],
    outputCol="features"
)
data = assembler.transform(df).select("features", "Price")

# 7Ô∏è‚É£ Divisi√≥n de datos en entrenamiento y prueba
train, test = data.randomSplit([0.8, 0.2], seed=42)

# 8Ô∏è‚É£ Entrenamiento del modelo
print("\n=== ü§ñ Entrenando modelo de regresi√≥n lineal ===")
lr = LinearRegression(featuresCol="features", labelCol="Price")
modelo = lr.fit(train)

# 9Ô∏è‚É£ Evaluaci√≥n
predicciones = modelo.transform(test)
evaluador = RegressionEvaluator(labelCol="Price", predictionCol="prediction", metricName="rmse")
rmse = evaluador.evaluate(predicciones)
r2 = modelo.summary.r2

print(f"\nüìà RMSE (error cuadr√°tico medio): {rmse:.2f}")
print(f"üìä R¬≤ (coeficiente de determinaci√≥n): {r2:.4f}")

# 10Ô∏è‚É£ Guardar resultados
predicciones.select("Price", "prediction").limit(20).show()
modelo.write().overwrite().save("/data/modelo_lineal_precio")
print("\n‚úÖ Modelo guardado en /data/modelo_lineal_precio")

# 11Ô∏è‚É£ Cierre
spark.stop()
print("\n=== ‚úÖ EXPERIMENTO FINALIZADO CORRECTAMENTE ===")
