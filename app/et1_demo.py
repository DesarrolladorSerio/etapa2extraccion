from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count

# --- Crear sesi√≥n Spark conectada al cl√∫ster ---
spark = SparkSession.builder \
    .appName("Etapa2_Exploracion") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

print("\n=== üîπ ETAPA 2: Exploraci√≥n de datos y herramientas ===\n")

# --- 1. EXTRACCI√ìN ---
df = spark.read.option("header", True).csv("/data/pp-sample-10k.csv", inferSchema=True)
print(f"Filas: {df.count()} | Columnas: {len(df.columns)}")
df.printSchema()

# --- 2. PROCESAMIENTO ---
# Limpiar nulos y mantener columnas relevantes
df_clean = df.dropna(subset=["Price", "PropertyType", "County"])

# Convertir precios a millones (solo para visualizar m√°s limpio)
df_clean = df_clean.withColumn("Price_M", col("Price") / 1_000_000)

# --- 3. AN√ÅLISIS ---
# Promedio de precio por tipo de propiedad y condado
stats = df_clean.groupBy("County", "PropertyType").agg(
    count("*").alias("Cantidad_ventas"),
    avg("Price_M").alias("Precio_promedio_M")
).orderBy(col("Precio_promedio_M").desc())

stats.show(100, truncate=False)

print("\n‚úÖ Proceso completado correctamente.\n")

spark.stop()
