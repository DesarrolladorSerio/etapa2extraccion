# ğŸ§  Etapa 2 â€“ ExploraciÃ³n de Datos y Herramientas
**Proyecto:** PredicciÃ³n de precios de viviendas con Apache Spark  
**Curso:** ExtracciÃ³n y GestiÃ³n de Datos Masivos 2025-2  
**Integrantes:** *[Agregar nombres del grupo]*  

---

## ğŸ“˜ DescripciÃ³n General
Este proyecto corresponde a la **Etapa 2**, cuyo objetivo es **demostrar que los datos elegidos son apropiados** para el anÃ¡lisis, verificando su **calidad, estructura y volumen**, ademÃ¡s de **probar las herramientas** que se usarÃ¡n en etapas posteriores (Spark MLlib en la Etapa 3).  

Se utiliza un **clÃºster distribuido de Apache Spark 4.0.1** con **3 nodos (1 master y 2 workers)** desplegado con **Docker Compose**.  
El flujo implementa las tres fases principales de un proceso ETL:  
1. **ExtracciÃ³n** del dataset.  
2. **TransformaciÃ³n y limpieza** de datos.  
3. **AnÃ¡lisis exploratorio** mediante agregaciones y estadÃ­sticas.  

---

## âš™ï¸ Requisitos previos
AsegÃºrate de tener instalado:
- **Docker Desktop** y **Docker Compose**
- **PowerShell** o **Terminal Bash**
- Archivo del proyecto con la siguiente estructura:

```
etapa2/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ app/
â”‚   â””â”€â”€ et1_demo.py
â””â”€â”€ data/
    â””â”€â”€ pp-sample-10k.csv
```

---

## ğŸ§© Arquitectura del ClÃºster

| Servicio | Rol | Imagen | Puertos expuestos |
|-----------|-----|--------|-------------------|
| spark-master | Nodo principal (control y monitoreo) | apache/spark:4.0.1 | 9090:8080, 7077:7077 |
| spark-worker-1 | Nodo de ejecuciÃ³n | apache/spark:4.0.1 | â€” |
| spark-worker-2 | Nodo de ejecuciÃ³n | apache/spark:4.0.1 | â€” |

ğŸ“¡ Red interna: `sparknet`  
ğŸ“ Carpetas compartidas:
- `/app` â†’ scripts de anÃ¡lisis  
- `/data` â†’ datasets CSV  

---

## ğŸš€ EjecuciÃ³n paso a paso

### 1ï¸âƒ£ Levantar el clÃºster
Desde la carpeta principal del proyecto:
```bash
docker compose up -d
```

Verifica los contenedores:
```bash
docker ps
```
DeberÃ­as ver tres contenedores activos:  
`spark-master`, `spark-worker-1`, `spark-worker-2`

Interfaz Web del master:  
ğŸ‘‰ [http://localhost:9090](http://localhost:9090)

---

### 2ï¸âƒ£ Ejecutar el script de anÃ¡lisis
Ejecuta el flujo ETL dentro del contenedor master:

```bash
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --master spark://spark-master:7077 /app/et1_demo.py"
```

Esto realiza:
- **ExtracciÃ³n:** lectura paralela del CSV `/data/pp-sample-10k.csv`.  
- **TransformaciÃ³n:** limpieza de filas nulas y conversiÃ³n de tipos.  
- **AnÃ¡lisis:** cÃ¡lculo de cantidad de ventas y precios promedio por tipo y regiÃ³n.  

El resultado aparece en consola y puede verse reflejado en la interfaz de Spark UI.

---

### 3ï¸âƒ£ Verificar la ejecuciÃ³n
Durante la ejecuciÃ³n se observarÃ¡n:
- *Logs* de tareas distribuidas (workers procesando en paralelo).  
- InformaciÃ³n del esquema y recuento de filas/columnas:
  ```
  Filas: 10000 | Columnas: 9
  root
   |-- Price: integer ...
   |-- County: string ...
  ```
- Tabla resumen:
  ```
  +------------------+------------+---------------+-----------------+
  |County            |PropertyType|Cantidad_ventas|Precio_promedio_M|
  +------------------+------------+---------------+-----------------+
  ```

---

### 4ï¸âƒ£ Detener el clÃºster
Al finalizar, puedes detener todos los contenedores:
```bash
docker compose down
```

---

## ğŸ“Š Resultados esperados

- Dataset correctamente cargado y procesado en Spark.  
- EstadÃ­sticas y promedios distribuidos calculados sin errores.  
- Funcionamiento confirmado del clÃºster (2 workers conectados al master).  
- Prueba de concepto de anÃ¡lisis exploratorio con datos reales.  

---

## ğŸ§  Conclusiones
- Los datos inmobiliarios del *Land Registry UK* presentan una estructura limpia, coherente y escalable.  
- Spark 4.0.1 demostrÃ³ manejar el dataset de forma eficiente en modo distribuido.  
- El flujo de **extracciÃ³n, limpieza y anÃ¡lisis** sienta las bases para las etapas siguientes (modelado predictivo con MLlib).  
